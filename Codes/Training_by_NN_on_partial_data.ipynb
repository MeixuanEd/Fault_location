{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import *\n",
    "from numpy import dot, multiply, diag, power\n",
    "from numpy import pi, exp, sin, cos, cosh, tanh, real, imag\n",
    "from numpy.linalg import inv, eig, pinv,norm\n",
    "from scipy.linalg import svd, svdvals\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from io import BytesIO\n",
    "from functools import partial \n",
    "from IPython.display import clear_output, Image, display, HTML  \n",
    "import scipy.io as sio \n",
    "import time  \n",
    "import re\n",
    "\n",
    "# parameters \n",
    "patience = 12\n",
    "n_classes=87 \n",
    "fault_type = 2; # 0--Tp; 1--LG; 2--LLG; 3--LL\n",
    "impe_type = 2; # 1~4 fault impedance increases\n",
    "model_name ='Test_results'#'NN_locate_30'#'NN_locate_half' 'NN_locate_full' \n",
    "# parameters for CNN \n",
    "lambda_loss_amount = 0.001\n",
    "learning_rate = 0.001\n",
    "training_iters =9000\n",
    "batch_size =50\n",
    "display_step = 1000 \n",
    "decay_c =0.9\n",
    "times = 1\n",
    "buses =68                     \n",
    "rootPath = './01_datasets/Iu_feature' \n",
    "trainName ='Line_faults_train'   \n",
    "testName = 'Line_faults_test' +'_type_' + str(fault_type) + '_'+str(impe_type)\n",
    "evalName = 'Line_faults_eval' \n",
    "data = sio.loadmat(os.path.join(rootPath, trainName))\n",
    "linedata = data['line']   \n",
    "\n",
    "def load_data(w,path,name): \n",
    "    global times, buses\n",
    "    import scipy.io as sio \n",
    "    PathName = os.path.join(path, name)\n",
    "    data=sio.loadmat(PathName); \n",
    "    dV= data['dV_feature'] \n",
    "    Y_ad= data['Y'] \n",
    "    train_data = (Y_ad[:,w] @ dV[w,:]).imag.T   \n",
    "    train_labels = data['y_num'] \n",
    "    col, buses = np.shape(train_data)  \n",
    "    train_x = np.float64(np.reshape(train_data, (int(col/times), buses)))  \n",
    "    train_y = np.zeros((int(col/times), n_classes))\n",
    "    for i in range(int(col/times)):\n",
    "        train_y[i,train_labels[0][i] ] = 1;\n",
    "    return train_x, train_y ,col\n",
    "\n",
    "\n",
    "def choose_w(line, thres):\n",
    "    global buses\n",
    "    all_freq =np.zeros((1,buses))\n",
    "    for i in range( buses):\n",
    "        ifreq = size(np.where(line[:,0] == i+1 )) + size(np.where(line[:,1] == i+1))\n",
    "        all_freq[0][i] = ifreq\n",
    "    w = [i for i in range(buses) if all_freq[0][i] >thres]  \n",
    "    return w\n",
    "\n",
    "# load data\n",
    "def load_all_data(w):\n",
    "    global train_data, train_labels, train_num, test_data, test_labels, test_num, eval_data, eval_labels, eval_num,samples,buses, times \n",
    "    train_data, train_labels, train_num = load_data(w,rootPath, trainName)  \n",
    "    eval_data, eval_labels,eval_num= load_data(w,rootPath, evalName)  \n",
    "    test_data, test_labels,test_num= load_data(w,rootPath, testName)   \n",
    "    print (np.shape(train_data))\n",
    "    print (np.shape(train_labels))\n",
    "    print (np.shape(eval_data))\n",
    "    print (np.shape(test_data))  \n",
    "def input_weight_all(Theta,name):# Theta is a list type\n",
    "    import pickle\n",
    "    filepointer=open(name,\"wb\")\n",
    "    pickle.dump(Theta,filepointer,protocol=2)\n",
    "    filepointer.close()\n",
    "    return \n",
    "\n",
    "def variable_summaries(var):\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean', mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "            tf.summary.scalar('stddev', stddev)\n",
    "            tf.summary.scalar('max', tf.reduce_max(var))\n",
    "            tf.summary.scalar('min', tf.reduce_min(var))\n",
    "            tf.summary.histogram('histogram', var) \n",
    "def NN_net(x,y ): # weights are dictionaries\n",
    "    global buses, times  \n",
    "    layer1 = int(buses/2); layer2 = int(layer1/2); layer3 = int(layer2/2); layer4 = int(layer3/2)\n",
    "    with tf.variable_scope('NN1'):\n",
    "        w1= tf.get_variable('w1',shape=[ buses, layer1],\n",
    "               initializer = tf.contrib.layers.xavier_initializer_conv2d(uniform=True, seed=None, dtype=tf.float32))  \n",
    "        b1=tf.get_variable( 'b1',\n",
    "          shape = [layer1],\n",
    "          initializer=tf.constant_initializer(0.0)) \n",
    "        # fully connected layer \n",
    "        fc1 =  tf.nn.relu(tf.add(tf.matmul(x, w1), b1))\n",
    "    with tf.variable_scope('NN2'):\n",
    "        w2= tf.get_variable('w2',shape=[ layer1, layer2],\n",
    "               initializer = tf.contrib.layers.xavier_initializer_conv2d(uniform=True, seed=None, dtype=tf.float32))  \n",
    "        b2=tf.get_variable( 'b2',\n",
    "          shape = [layer2],\n",
    "          initializer=tf.constant_initializer(0.0)) \n",
    "        # fully connected layer \n",
    "        fc2 =  tf.nn.relu(tf.add(tf.matmul(fc1, w2), b2)) \n",
    "    with tf.variable_scope('Out'):\n",
    "        wout= tf.get_variable('wout',shape=[ layer2,n_classes],\n",
    "               initializer = tf.contrib.layers.xavier_initializer_conv2d(uniform=True, seed=None, dtype=tf.float32))  \n",
    "        bout=tf.get_variable( 'bout',\n",
    "          shape = [n_classes],\n",
    "          initializer=tf.constant_initializer(0.0)) \n",
    "        # fully connected layer \n",
    "        fout =   tf.add(tf.matmul(fc2, wout), bout)  \n",
    "    return fout\n",
    "\n",
    "def establish_model(): \n",
    "    global  learning_rate, training_iters,display_step,batch_size,patience \n",
    "    global buses, times, train_data, train_labels, eval_data, eval_labels, test_data, test_labels\n",
    "    tf.reset_default_graph()\n",
    "    x = tf.placeholder(tf.float32, [None, buses ])\n",
    "    y = tf.placeholder(tf.int32, [None, n_classes])   \n",
    "    pred = NN_net(  x, y ) \n",
    "    y_score = tf.nn.softmax(pred)\n",
    "    predict_op=tf.argmax(tf.nn.softmax(pred), 1)  \n",
    "    # Define loss and optimizer \n",
    "    with tf.name_scope('loss'):   \n",
    "        l2 = lambda_loss_amount * sum(\n",
    "        tf.nn.l2_loss(tf_var) for tf_var in tf.trainable_variables() ) \n",
    "        cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=pred, labels=y))+l2  \n",
    "    with tf.name_scope('Optimizer'):  \n",
    "        optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate , decay=decay_c).minimize(cost) \n",
    "    with tf.name_scope('err'): \n",
    "        correct = tf.equal(predict_op, tf.argmax(y, 1))\n",
    "        err=1- tf.reduce_mean(tf.cast(correct, tf.float32))  \n",
    "    tf.summary.scalar('err',err)\n",
    "    tf.summary.scalar('loss',cost)\n",
    "    \n",
    "    #save\n",
    "    saver = tf.train.Saver()\n",
    "    # Launch the graph\n",
    "    sess = tf.InteractiveSession()\n",
    "    # Merge all the summaries and write them out to C:/Users/Lab/\n",
    "    merged = tf.summary.merge_all()\n",
    "    train_writer = tf.summary.FileWriter('./train',\n",
    "                                      sess.graph)\n",
    "    test_writer = tf.summary.FileWriter('./test') \n",
    "    train_writer.add_graph(sess.graph)\n",
    "    # training\n",
    "    step=1\n",
    "    loss_list=[]\n",
    "    train_rate=[]\n",
    "    eval_rate=[]\n",
    "    n_incr_num =0\n",
    "    best_loss = np.Inf \n",
    "    # Initializing the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    while step < training_iters:\n",
    "        ind = np.arange(train_data.shape[0])\n",
    "        batch_idx = np.random.choice(ind, batch_size, replace=False) # the replace means not allow to pick up the same element again\n",
    "        batch_x = train_data[batch_idx] \n",
    "        batch_y= train_labels[batch_idx] \n",
    "        indeval = np.arange(eval_data.shape[0]) \n",
    "        eval_idx = np.random.choice(indeval, batch_size, replace=False)\n",
    "        batch_xeval=eval_data[eval_idx] \n",
    "        batch_yeval=eval_labels[eval_idx] \n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(optimizer, feed_dict={x: batch_x,  y: batch_y }) \n",
    "        loss, train_err = sess.run([cost, err], feed_dict={x: batch_x,  y: batch_y  })\n",
    "        loss_eval,eval_err=sess.run([cost ,err ], feed_dict={x: batch_xeval, y: batch_yeval })\n",
    "        loss_list.append(loss)\n",
    "        train_rate.append(train_err)\n",
    "        eval_rate.append(eval_err)\n",
    "        step += 1\n",
    "        if step % display_step == 0:\n",
    "            # Calculate batch loss and err \n",
    "            print(\"Iter \" + str(step ) + \", Minibatch Loss= \" +  \"{:.2f}\".format(loss) + \",training err= \" + \"{:.2f}\".format(train_err)+ \",validating err= \" + \"{:.2f}\".format(eval_err))   \n",
    "            #print ('learning_rate is', learning_rate.eval())\n",
    "            if loss_eval < best_loss:\n",
    "                best_loss = loss_eval\n",
    "                n_incr_num =0\n",
    "            else:\n",
    "                n_incr_num+=1\n",
    "            if (n_incr_num >= patience) and (step > 7000):\n",
    "                print ('Early_stopping! and the iterations is', step)\n",
    "                # Create the collection.\n",
    "                tf.get_collection(\"validation_nodes\")\n",
    "                # Add stuff to the collection.\n",
    "                tf.add_to_collection(\"validation_nodes\", x)  \n",
    "                tf.add_to_collection(\"validation_nodes\", y) \n",
    "                tf.add_to_collection(\"validation_nodes\", y_score) \n",
    "                results = predict_op\n",
    "                save_path = saver.save(sess, \"./train/\"+model_name)\n",
    "                correct_results_true, results_true=sess.run([correct, predict_op] , feed_dict={x:test_data, y:test_labels }) \n",
    "                total_test_true=sess.run(err , feed_dict={x:test_data,   y:test_labels })  \n",
    "                print(\"Testing err :\", total_test_true) \n",
    "                print ('Accuracy is', 100*(1-total_test_true))\n",
    "                print ('Early Stop!')\n",
    "                return loss_list,step,train_rate,eval_rate, correct_results_true, results_true, total_test_true\n",
    "    # Create the collection.\n",
    "    tf.get_collection(\"validation_nodes\")\n",
    "    # Add stuff to the collection.\n",
    "    tf.add_to_collection(\"validation_nodes\", x)   \n",
    "    tf.add_to_collection(\"validation_nodes\", y) \n",
    "    tf.add_to_collection(\"validation_nodes\", y_score) \n",
    "    save_path = saver.save(sess, \"./train/\"+model_name)\n",
    "    correct_results_true, results_true=sess.run([correct, predict_op] ,  feed_dict={x:test_data, y:test_labels }) \n",
    "    total_test_true=sess.run(err , feed_dict={x:test_data,   y:test_labels })           \n",
    "    print(\"Testing err of subtract true :\", total_test_true) \n",
    "    print ('Accuracy is', 100*(1-total_test_true))\n",
    "    print(\"Optimization Finished!\") \n",
    "    return loss_list,step,train_rate,eval_rate, correct_results_true, results_true, total_test_true  \n",
    "def each_perform(correct_results,eval_labels ):\n",
    "    label_y=eval_labels#np.argmax(eval_labels,1)\n",
    "    acc_rate = np.zeros((1,n_classes ))\n",
    "    for i in range(n_classes):\n",
    "        location = np.where(label_y == i)  \n",
    "        correct = [correct_results[j] for j in location[0]]  \n",
    "        if len(correct) > 0:\n",
    "            acc_rate[0][i] = 100*np.mean(correct)  \n",
    "    return acc_rate\n",
    "\n",
    "def plot_loss(loss,train_step,from_second,name_save, plot_name,plot_title):\n",
    "    if from_second :\n",
    "        plt.plot(range(0,train_step-1,1),loss[1:])\n",
    "    else:\n",
    "        plt.plot(range(0,train_step,1),loss[0:])\n",
    "    plt.xlabel('Iterative times (t)')\n",
    "    plt.ylabel(plot_name)\n",
    "    plt.title(plot_title)\n",
    "    plt.grid(True)\n",
    "    plt.savefig(name_save)\n",
    "    plt.show()\n",
    "def strip_consts(graph_def, max_const_size=32):\n",
    "    \"\"\"Strip large constant values from graph_def.\"\"\"\n",
    "    strip_def = tf.GraphDef()\n",
    "    for n0 in graph_def.node:\n",
    "        n = strip_def.node.add() \n",
    "        n.MergeFrom(n0)\n",
    "        if n.op == 'Const':\n",
    "            tensor = n.attr['value'].tensor\n",
    "            size = len(tensor.tensor_content)\n",
    "            if size > max_const_size:\n",
    "                tensor.tensor_content = \"<stripped %d bytes>\"%size\n",
    "    return strip_def\n",
    "\n",
    "def show_graph(graph_def, max_const_size=32):\n",
    "    \"\"\"Visualize TensorFlow graph.\"\"\"\n",
    "    if hasattr(graph_def, 'as_graph_def'):\n",
    "        graph_def = graph_def.as_graph_def()\n",
    "    strip_def = strip_consts(graph_def, max_const_size=max_const_size)\n",
    "    code = \"\"\"\n",
    "        <script>\n",
    "          function load() {{\n",
    "            document.getElementById(\"{id}\").pbtxt = {data};\n",
    "          }}\n",
    "        </script>\n",
    "        <link rel=\"import\" href=\"https://tensorboard.appspot.com/tf-graph-basic.build.html\" onload=load()>\n",
    "        <div style=\"height:600px\">\n",
    "          <tf-graph-basic id=\"{id}\"></tf-graph-basic>\n",
    "        </div>\n",
    "    \"\"\".format(data=repr(str(strip_def)), id='graph'+str(np.random.rand()))\n",
    "\n",
    "    iframe = \"\"\"\n",
    "        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"{}\"></iframe>\n",
    "    \"\"\".format(code.replace('\"', '&quot;'))\n",
    "    display(HTML(iframe))    \n",
    "\n",
    "\n",
    "def main():\n",
    "    thres =4 \n",
    "    w = choose_w(linedata, thres)\n",
    "    w = np.r_[w, 23,42,61,51,57,6,37,58,45,26,10,33,4,41,8]  \n",
    "    load_all_data(w)\n",
    "    loss,step,train_rate,eval_rate, correct_results_true, results_true, total_test_true  =establish_model()  \n",
    "    #plot_loss(loss,step-1,False,\"Loss_value.png\", 'Loss','Loss function value with iterations') \n",
    "    #plot_loss(train_rate,step-1,False,\"Train_err_rate.png\",'Training accurate rate','Training classification with iterations')\n",
    "    #plot_loss(eval_rate,step-1,False,\"Eval_err_rate.png\",'Evaluating accurate rate','Evaluating classification with iterations')  \n",
    "    total_acc= (100*(1-total_test_true))\n",
    "    print (total_acc)\n",
    "    show_graph(tf.get_default_graph().as_graph_def()) \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
